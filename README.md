# ifscboulderscraper
On my journey to trying to apply some statistical and machine learning techniques I have obtained through my postgraduate studies towards actuarial science, I wanted to build some models surrounding the IFSC/World Climbing bouldering competitions. As a result, I needed to obtain all the relevant data from the website by building a web scraper. My goal is to create some models which would indicate the most influential factors towards a podium result.

My initial plan was to simply manually collect data (which I knew would be time-consuming) from both https://ifsc.results.info/ and https://www.sportclimbingstats.com/. It was tedious but I didn't know how else to collect the data at this stage. This process took a few weeks and, by the end of it, I wasn't sure if I was satisfied with the data I had collected. I had wished to obtain more detailed data but I didn't know what else I could do.

A few months later, I had landed an internship at IAG which was incredibly useful and I am grateful for the experience and knowledge I obtained from it, allowing me to understand more about myself and my potential in the role of an actuary. One of the tasks I was given was to collect quotes from other insurance companies. Like the initial climbing data, I did this manually. Then one of my mentors, who also coincidentally was a guest lecture for one of my courses, saw what I was doing and said "I reckon you could build a web scraper to do that", to which I nodded and thought to myself "What is a web scraper?". I guess for most actuaries or computer scientists, this was a no brainer but my background was in education as a high school mathematics teacher and I had no prior experience with any of this coding stuff.

After my internship ended, I was on my summer break and decided to pick up where I left off with my climbing data collection. So I looked into what my mentor said and thought "Wow this is hard but I think this is my best shot at getting all the data I want". Then began my back-and-forth relationship with ChatGPT to start building this web scraper from reference to only the official results website https://ifsc.results.info/. I decided to start scraping results from 2008 as the format of the results were consistent with the current recording of competition results.

Initially, ChatGPT was telling me to use APIs to try to scrape the data, and all the attempts to implement this failed. I investigated this issue deeper and found that the IFSC results website had made their APIs private, which meant I needed to use another method to collect the data. Then came Chromote. I will be the first to admit that I don't completely understand every bit of code, but I think I learnt the general gist. The Chromote package in R seems to create its own browser and actually go to the webpages through that browser to scrape the page. I then learnt that to identify the details I needed, it required me to specify the elements of the webpage. What I didn't realise was this scraping process would take a very long time, but I had time so I was happy to keep working on this scraper. The code went through many iterations. I checked the results that were obtained, saw errors, investigated the cause, then adjusted the code with the help of ChatGPT to address the error. I think one of the largest errors that I identified was that the scraper was incorrectly skipping boulder events i.e. my dataset was meant to be larger. Turns out that the Chromote package also has the ability to click on tabs in the webpage, which was the cause of many errors. I think after this, most of the code was relatively smooth sailing, I successfully made the code click on the correct tabs to get the correct data, and I had fine-tuned the parameters enough to be quite confident that the data collected was accurate.

I also thought it to be useful to create a code which would extract single events, especially when trying to update the current results with new competitions so that it would not waste time trying to extract data from already scraped events. From this process, I found some events that were incorrectly missed and was able to use this code to add these few missed events to the overall results.

I later met up with another mentor from IAG to just catch up. While discussing my journey in collecting this data, he mentioned that Python would have been a lot easier. So I guess I will keep that in mind for next time because I haven't used Python before.

Overall, I am pleased with what has come from this little project. I don't feel like I am completely capable of developing the code for a web scraper from scratch, but it has taught my where to start and look to complete projects, and has improved my technical skills and understanding. I acknowledge that this is likely not the most efficient code, but it works and that is all I wanted from it.

I would like to acknowledge the assistance provided by ChatGPT [Large Language Model] which offered specific code and editorial suggestions.
